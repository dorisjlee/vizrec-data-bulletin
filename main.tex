\documentclass[11pt]{article}
\usepackage{deauthor}
\usepackage{times}
\usepackage{graphicx}
\usepackage{xspace}
\usepackage{xcolor}
\usepackage[square,numbers]{natbib}

\newenvironment{denselist}{
    \begin{list}{\small{$\bullet$}}%
    {\setlength{\itemsep}{0ex} \setlength{\topsep}{0ex}
    \setlength{\parsep}{0pt} \setlength{\itemindent}{0pt}
    \setlength{\leftmargin}{1.5em}
    \setlength{\partopsep}{0pt}}}%
    {\end{list}}

\newcommand{\squishlist}{
   \begin{list}{$\bullet$}
    { \setlength{\itemsep}{0pt}
      \setlength{\parsep}{2pt}
      \setlength{\topsep}{0pt}
      \setlength{\partopsep}{0pt}
      \leftmargin=25pt
\rightmargin=0pt
\labelsep=5pt
\labelwidth=10pt
\itemindent=0pt
\listparindent=0pt
\itemsep=\parsep
    }
}
\newcommand{\squishend}{\end{list}}

% use extensively to toggle between paper and TR
\newcommand{\eat}[1]{}
% \newcommand{\papertext}[1]{{\leavevmode\color{blue}{#1}}}
% \newcommand{\techreport}[1]{{\leavevmode\color{red}{#1}}}
\newcommand{\papertext}[1]{#1}
\newcommand{\techreport}[1]{}
\newcommand{\boldpara}[1]{\textbf{\paragraph{#1}}}
% de-facto paragraph format
\newcommand{\stitle}[1]{\par\noindent\textbf{#1}}
\newcommand{\tvcg}[1]{{\leavevmode\color{blue}{#1}}}
\newcommand{\cut}[1]{{\leavevmode\color{lightgray}{#1}}}
\newcommand{\ccut}[1]{} %confirmed cut
\def\plainauthor{Doris Jung-Lin Lee, Aditya Parameswaran}
\def\emptyauthor{} 
\def\plainkeywords{Data visualization, exploratory data analysis, visual querying.}
\def\plaingeneralterms{Documentation, Standardization}

\newcommand{\zv}{\textit{zenvisage}\xspace}
\newcommand{\vida}{\textsc{vida}\xspace}
\newcommand{\sbd}{\textsc{Storyboard}\xspace}
\newcommand{\agp}[1]{\textcolor{green}{Aditya: #1}}
\newcommand{\dor}[1]{\textcolor{blue}{Doris: #1}} 

\newcommand\notes[1]{\textcolor{red}{#1}}

% To make various LaTeX processors do the right thing with page size.
\def\pprw{8.5in}
\def\pprh{11in}
\special{papersize=\pprw,\pprh}
\setlength{\paperwidth}{\pprw}
\setlength{\paperheight}{\pprh}
\setlength{\pdfpagewidth}{\pprw}
\setlength{\pdfpageheight}{\pprh}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\title{Navigating Visualization Collections}
\title{Towards a Holistic Workflow for Visual Data Exploration}
\author{\plainauthor\\
\{jlee782,adityagp\}@illinois.edu\\
University of Illinois, Urbana-Champaign}
\maketitle
\begin{abstract}

Visualization is one of the 
most effective and widely-used 
techniques for understanding data. 
Yet, the growing use of visualizations 
for exploratory data analysis 
presses new demands beyond simply 
the graphical presentation 
and visualization authoring 
capabilities offered in existing tools. 
In particular, many data analysis
tasks involve navigating large 
collections of visualizations to make sense
of trends in data;
at present, this navigation is done manually or
programmatically. 
We outline a vision for an
intelligent, interactive, understandable,
and usable tool 
that can help automate
this largely manual navigation: we call our tool \vida\footnote{Vida is short for {\em life} in Spanish.}---short for {VIsual Discovery Assistant}. 
We argue that typical navigation tasks can be 
organized across two dimensions---overall goal 
and precision of specification.
We organize prior work---both our own work, as well
as other ongoing work in this area---across 
these two dimensions, and highlight
new research challenges.
Together, addressing these challenges underlying \vida
can help pave the way for a comprehensive
solution for removing the pain points
in visual data exploration.
% We outline a vision for a new breed of 
% interactive tools that can help automate
% this largely manual navigation process. 
% We argue that these tools need to span two 
% dimensions---precision and task.
% We organize prior work---both our own work, as well
% as other ongoing work in this area---to highlight
% new research challenges
% across these two dimensions.
% Together, addressing these challenges
% can help provide a comprehensive solution 
% for navigating and making sense of 
% large collections of visualizations. 
% In this paper, we describe three areas of visual data exploration facing these challenges. First, we introduce the problem of precise querying: how to search for a desired trend or pattern given the large space of visualizations that could be generate from a given dataset. As vague and complex queries cannot often be addressed through interactions in precise visual querying systems, we introduce a class of intelligent visual querying systems that allows users provide feedback in order to clarify and refine their queries. Finally, even with a omnipotent system that addresses any given visual query, users may not know what to query. Therefore, we advocate for recommendations that facillitates distributional and contextual awareness to help users gain more holistic data understanding, guide them towards meaningful insights, and jumpstart further inquiries. We describe the exemplary systems drawing from both our own work (\zv and \sbd) and ongoing research in the area to highlight future directions and opportunties in this space.
% This paper surveys the emerging field of formally reasoning
% about and optimizing open-ended crowdsourcing, a popular and crucially important, but severely
% understudied class of crowdsourcing—the next frontier in crowdsourced data management. The underlying
% challenges include distilling the right answer when none of the workers agree with each other,
% teasing apart the various perspectives adopted by workers when answering tasks, and effectively selecting
% between the many open-ended operators appropriate for a problem. We describe the approaches that
% we’ve found to be effective for open-ended crowdsourcing, drawing from our experiences in this space.
\end{abstract}

% Eventually, we need to compile all of these into one single file  during submission time. 
\input{0-Introduction}
\input{1-PreciseSearch}

\input{2-HypothesisFormation}
\input{3-IntelligentSearch}
\input{4-DatasetUnderstanding}
\input{5-Conclusion}
\section*{Acknowledgement}
A. P. acknowledges support from grants IIS-1513407 and IIS-1633755 awarded by the National Science Foundation, grant 1U54GM114838 awarded by NIGMS and 3U54EB020406-02S1 awarded by NIBIB through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and funds from Adobe, Google, and the Siebel Energy Institute. The content is solely the responsibility of the
authors and does not necessarily represent the official views of the funding agencies and organizations. \dor{copied this from the crowdsourcing vision paper, might have to change grant number and sources accrodingly. Might also want to include acknowledgement for collaborators here too.}
{\footnotesize \bibliographystyle{named}
\bibliography{reference}}
\end{document}
